{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os, pickle, numpy\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Train and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Train data len: 10000\n",
      "----------------------------------------------------------------------\n",
      "Test data len: 1000\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open('train_qa.txt','rb') as file:\n",
    "    train_data = pickle.load(file)\n",
    "print('----------------------------------------------------------------------')\n",
    "print(f'Train data len: {len(train_data)}')\n",
    "\n",
    "with open('test_qa.txt','rb') as file:\n",
    "    test_data = pickle.load(file)\n",
    "print('----------------------------------------------------------------------')\n",
    "print(f'Test data len: {len(test_data)}')\n",
    "print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 3\n",
      "----------------------------------------------------------------------\n",
      "Story: Mary moved to the bathroom . Sandra journeyed to the bedroom . Mary went back to the bedroom . Daniel went back to the hallway . Sandra went to the kitchen . Daniel went back to the bathroom .\n",
      "----------------------------------------------------------------------\n",
      "Question: Is Daniel in the office ?\n",
      "----------------------------------------------------------------------\n",
      "Answer: no\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "print(f'Set {x}')\n",
    "print('----------------------------------------------------------------------')\n",
    "print('Story:',' '.join(train_data[x-1][0]))\n",
    "print('----------------------------------------------------------------------')\n",
    "print('Question:',' '.join(train_data[x-1][1]))\n",
    "print('----------------------------------------------------------------------')\n",
    "print('Answer:',train_data[x-1][2])\n",
    "print('----------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is unique for this particular dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = test_data + train_data\n",
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words in questions and stories: 37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '?',\n",
       " 'Daniel',\n",
       " 'Is',\n",
       " 'John',\n",
       " 'Mary',\n",
       " 'Sandra',\n",
       " 'apple',\n",
       " 'back',\n",
       " 'bathroom',\n",
       " 'bedroom',\n",
       " 'discarded',\n",
       " 'down',\n",
       " 'dropped',\n",
       " 'football',\n",
       " 'garden',\n",
       " 'got',\n",
       " 'grabbed',\n",
       " 'hallway',\n",
       " 'in',\n",
       " 'journeyed',\n",
       " 'kitchen',\n",
       " 'left',\n",
       " 'milk',\n",
       " 'moved',\n",
       " 'no',\n",
       " 'office',\n",
       " 'picked',\n",
       " 'put',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'took',\n",
       " 'travelled',\n",
       " 'up',\n",
       " 'went',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "# add unique words to vocabulary\n",
    "for story, question, answer in all_data:\n",
    "    vocabulary = vocabulary.union(set(story))\n",
    "    vocabulary = vocabulary.union(set(question))\n",
    "\n",
    "vocabulary.add('no')\n",
    "vocabulary.add('yes')\n",
    "\n",
    "vocab_size = len(vocabulary)+1 # +1 because in keras paddind function it is required to have a placeholder\n",
    "\n",
    "print('Total number of unique words in questions and stories:',vocab_size-1)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the longest story and longest question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max story length is: 156 words\n",
      "Max question length is: 6 words\n"
     ]
    }
   ],
   "source": [
    "stories = []\n",
    "questions = []\n",
    "\n",
    "for story, question, answer in all_data:\n",
    "    stories.append(len(story))\n",
    "    questions.append(len(question))\n",
    "\n",
    "max_story_len = max(stories)\n",
    "max_question_len = max(questions)\n",
    "\n",
    "print('Max story length is:', max_story_len,'words')\n",
    "print('Max question length is:',max_question_len,'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down': 1, 'sandra': 2, '.': 3, 'in': 4, 'garden': 5, 'kitchen': 6, 'back': 7, 'up': 8, 'football': 9, 'travelled': 10, 'got': 11, 'the': 12, 'office': 13, 'apple': 14, 'daniel': 15, '?': 16, 'picked': 17, 'yes': 18, 'there': 19, 'bedroom': 20, 'john': 21, 'grabbed': 22, 'no': 23, 'mary': 24, 'bathroom': 25, 'to': 26, 'left': 27, 'journeyed': 28, 'hallway': 29, 'went': 30, 'took': 31, 'milk': 32, 'dropped': 33, 'put': 34, 'moved': 35, 'is': 36, 'discarded': 37}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters=[]) # because all sighs are important\n",
    "tokenizer.fit_on_texts(vocabulary)\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "# save custom tokenizer\n",
    "import pandas as pd\n",
    "pd.DataFrame([tokenizer.word_index.keys(),tokenizer.word_index.values()]).T.rename({1:'word_index', 0:'word'},axis=1).to_csv('./model_checkpoints/tokenizer.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def vectorize_data(data,\n",
    "                    word_index,\n",
    "                    max_story_len,\n",
    "                    max_question_len):\n",
    "\n",
    "    stories = []\n",
    "    questions = []\n",
    "\n",
    "    answers = []\n",
    "\n",
    "    for story, question, answer in data:\n",
    "        stories_part = [word_index[word.lower()] for word in story]         # return index of each word according to their position in word index for stories\n",
    "        questions_part = [word_index[word.lower()] for word in question]    # return index of each word according to their position in word index for questions\n",
    "\n",
    "        answers_part = np.zeros(len(word_index)+1)                          # placeholder\n",
    "        answers_part[word_index[answer]] = 1                                # in the index position of 'yes' or 'no' put 1 \n",
    "\n",
    "        stories.append(stories_part)\n",
    "        questions.append(questions_part)\n",
    "        answers.append(answers_part)\n",
    "\n",
    "    return (pad_sequences(stories,maxlen=max_story_len),pad_sequences(questions,maxlen=max_question_len),np.array(answers))     # return padded data\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create padded train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, questions_train, answers_train = vectorize_data(train_data,word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len)\n",
    "inputs_test, questions_test, answers_test = vectorize_data(test_data,word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_train shape: (10000, 156) questions_train shape: (10000, 6) answers_train shape: (10000, 38)\n",
      "\n",
      "inputs_test shape: (1000, 156) questions_test shape: (1000, 6) answers_test shape: (1000, 38)\n"
     ]
    }
   ],
   "source": [
    "print('inputs_train shape:',inputs_train.shape, 'questions_train shape:',questions_train.shape,'answers_train shape:', answers_train.shape)\n",
    "print()\n",
    "print('inputs_test shape:',inputs_test.shape, 'questions_test shape:',questions_test.shape,'answers_test shape:', answers_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and instatiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"memory_network2_linux\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 156)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, None, 64)     2432        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 6, 64)        2432        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 156, 6)       0           sequential[0][0]                 \n",
      "                                                                 sequential_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 156, 6)       0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, None, 6)      228         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 156, 6)       0           activation[0][0]                 \n",
      "                                                                 sequential_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 6, 156)       0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6, 220)       0           permute[0][0]                    \n",
      "                                                                 sequential_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 32)           32384       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32)           0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 38)           1254        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 38)           0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 38,730\n",
      "Trainable params: 38,730\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 13:29:10.265728: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-07-22 13:29:10.265921: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-22 13:29:10.266756: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from memory_network import memory_network\n",
    "\n",
    "model = memory_network(max_story_len,max_question_len,vocab_size,optimizer='rmsprop',model_name='memory_network')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "            keras.callbacks.ModelCheckpoint(filepath=f'model_checkpoints/{model.name}.h5',save_best_only=True),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=10, factor=0.1, verbose=2, min_lr=1e-6),\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss',patience=15)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training is surprisingly tricky part, remember not to train using GPU, model has to be trained in a sequential manner not parallel, so use CPU instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-22 13:29:13.659118: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-07-22 13:29:13.676404: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3600000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1200\n",
      "313/313 [==============================] - 7s 16ms/step - loss: 1.2740 - accuracy: 0.4805 - val_loss: 0.6989 - val_accuracy: 0.4970\n",
      "Epoch 2/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.7088 - accuracy: 0.5155 - val_loss: 0.6936 - val_accuracy: 0.5030\n",
      "Epoch 3/1200\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.6976 - accuracy: 0.4989 - val_loss: 0.6937 - val_accuracy: 0.4970\n",
      "Epoch 4/1200\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6954 - accuracy: 0.5043 - val_loss: 0.6940 - val_accuracy: 0.4970\n",
      "Epoch 5/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.6945 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.5030\n",
      "Epoch 6/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.6957 - accuracy: 0.4916 - val_loss: 0.6933 - val_accuracy: 0.4720\n",
      "Epoch 7/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.6946 - accuracy: 0.5002 - val_loss: 0.6935 - val_accuracy: 0.4700\n",
      "Epoch 8/1200\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.6938 - accuracy: 0.5031 - val_loss: 0.6944 - val_accuracy: 0.4850\n",
      "Epoch 9/1200\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.6921 - accuracy: 0.5087 - val_loss: 0.6888 - val_accuracy: 0.5420\n",
      "Epoch 10/1200\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.6805 - accuracy: 0.5611 - val_loss: 0.6190 - val_accuracy: 0.7200\n",
      "Epoch 11/1200\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 0.5937 - accuracy: 0.6940 - val_loss: 0.4641 - val_accuracy: 0.7970\n",
      "Epoch 12/1200\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 0.4808 - accuracy: 0.7878 - val_loss: 0.4307 - val_accuracy: 0.8310\n",
      "Epoch 13/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.4307 - accuracy: 0.8220 - val_loss: 0.4103 - val_accuracy: 0.8140\n",
      "Epoch 14/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.4091 - accuracy: 0.8301 - val_loss: 0.4023 - val_accuracy: 0.8290\n",
      "Epoch 15/1200\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3945 - accuracy: 0.8417 - val_loss: 0.4080 - val_accuracy: 0.8370\n",
      "Epoch 16/1200\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3653 - accuracy: 0.8519 - val_loss: 0.3945 - val_accuracy: 0.8250\n",
      "Epoch 17/1200\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.3684 - accuracy: 0.8471 - val_loss: 0.3793 - val_accuracy: 0.8300\n",
      "Epoch 18/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3613 - accuracy: 0.8518 - val_loss: 0.3812 - val_accuracy: 0.8430\n",
      "Epoch 19/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.3428 - accuracy: 0.8546 - val_loss: 0.3864 - val_accuracy: 0.8390\n",
      "Epoch 20/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3398 - accuracy: 0.8624 - val_loss: 0.3630 - val_accuracy: 0.8370\n",
      "Epoch 21/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.3342 - accuracy: 0.8637 - val_loss: 0.3681 - val_accuracy: 0.8330\n",
      "Epoch 22/1200\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 0.3274 - accuracy: 0.8681 - val_loss: 0.3527 - val_accuracy: 0.8390\n",
      "Epoch 23/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3236 - accuracy: 0.8622 - val_loss: 0.3567 - val_accuracy: 0.8440\n",
      "Epoch 24/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.3286 - accuracy: 0.8617 - val_loss: 0.3571 - val_accuracy: 0.8460\n",
      "Epoch 25/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3187 - accuracy: 0.8622 - val_loss: 0.3452 - val_accuracy: 0.8410\n",
      "Epoch 26/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.3073 - accuracy: 0.8644 - val_loss: 0.3521 - val_accuracy: 0.8410\n",
      "Epoch 27/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3271 - accuracy: 0.8602 - val_loss: 0.3454 - val_accuracy: 0.8370\n",
      "Epoch 28/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2982 - accuracy: 0.8742 - val_loss: 0.3377 - val_accuracy: 0.8340\n",
      "Epoch 29/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3153 - accuracy: 0.8609 - val_loss: 0.3589 - val_accuracy: 0.8430\n",
      "Epoch 30/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3143 - accuracy: 0.8626 - val_loss: 0.3612 - val_accuracy: 0.8510\n",
      "Epoch 31/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3069 - accuracy: 0.8619 - val_loss: 0.3437 - val_accuracy: 0.8390\n",
      "Epoch 32/1200\n",
      "313/313 [==============================] - 5s 17ms/step - loss: 0.3101 - accuracy: 0.8583 - val_loss: 0.3583 - val_accuracy: 0.8420\n",
      "Epoch 33/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2920 - accuracy: 0.8711 - val_loss: 0.3333 - val_accuracy: 0.8320\n",
      "Epoch 34/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2951 - accuracy: 0.8744 - val_loss: 0.3446 - val_accuracy: 0.8490\n",
      "Epoch 35/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.3069 - accuracy: 0.8642 - val_loss: 0.3394 - val_accuracy: 0.8420\n",
      "Epoch 36/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2960 - accuracy: 0.8697 - val_loss: 0.3370 - val_accuracy: 0.8450\n",
      "Epoch 37/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2897 - accuracy: 0.8724 - val_loss: 0.3400 - val_accuracy: 0.8400\n",
      "Epoch 38/1200\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 0.2917 - accuracy: 0.8712 - val_loss: 0.3364 - val_accuracy: 0.8430\n",
      "Epoch 39/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.2881 - accuracy: 0.8761 - val_loss: 0.3453 - val_accuracy: 0.8370\n",
      "Epoch 40/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2842 - accuracy: 0.8784 - val_loss: 0.3433 - val_accuracy: 0.8430\n",
      "Epoch 41/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.2943 - accuracy: 0.8729 - val_loss: 0.3454 - val_accuracy: 0.8430\n",
      "Epoch 42/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2940 - accuracy: 0.8707 - val_loss: 0.3371 - val_accuracy: 0.8330\n",
      "Epoch 43/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2880 - accuracy: 0.8679 - val_loss: 0.3501 - val_accuracy: 0.8410\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 44/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.2865 - accuracy: 0.8755 - val_loss: 0.3426 - val_accuracy: 0.8360\n",
      "Epoch 45/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2805 - accuracy: 0.8776 - val_loss: 0.3443 - val_accuracy: 0.8370\n",
      "Epoch 46/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2837 - accuracy: 0.8733 - val_loss: 0.3463 - val_accuracy: 0.8390\n",
      "Epoch 47/1200\n",
      "313/313 [==============================] - 4s 14ms/step - loss: 0.2697 - accuracy: 0.8821 - val_loss: 0.3426 - val_accuracy: 0.8400\n",
      "Epoch 48/1200\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.2715 - accuracy: 0.8786 - val_loss: 0.3471 - val_accuracy: 0.8370\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([inputs_train, questions_train], \n",
    "                    answers_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=1200,\n",
    "                    validation_data=([inputs_test, questions_test], \n",
    "                    answers_test),\n",
    "                    callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check what model has learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('./model_checkpoints/memory_network.h5')\n",
    "qa_tokenizer = pd.read_csv('model_checkpoints/tokenizer.csv',dtype={'word':str})\n",
    "qa_tokenizer = dict(list(zip(qa_tokenizer['word'],qa_tokenizer['word_index'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question():\n",
    "    import random\n",
    "\n",
    "    x = random.randint(a=1,b=len(test_data))\n",
    "\n",
    "    print('Story:',' '.join(test_data[x-1][0]))\n",
    "    s = test_data[x-1][0]\n",
    "    print('Question:',' '.join(test_data[x-1][1]))\n",
    "    q = test_data[x-1][1]\n",
    "    print('Answer:',test_data[x-1][2])\n",
    "    a = test_data[x-1][2]\n",
    "\n",
    "    return s, q, a, [(test_data[x-1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: Mary travelled to the hallway . Daniel got the apple there . Daniel journeyed to the bathroom . Mary moved to the kitchen . Daniel travelled to the hallway . Daniel dropped the apple .\n",
      "Question: Is Mary in the bedroom ?\n",
      "Answer: no\n",
      "Predicted answer is:  no\n",
      "Probability of certainty was:  99.41 %\n"
     ]
    }
   ],
   "source": [
    "_,_,_,question = generate_question()\n",
    "\n",
    "my_story,my_ques,my_ans = vectorize_data(question, qa_tokenizer,max_story_len,max_question_len)\n",
    "\n",
    "pred_results = model.predict(([ my_story, my_ques]))\n",
    "\n",
    "yes_prob = pred_results[0][qa_tokenizer['yes']]\n",
    "no_prob = pred_results[0][qa_tokenizer['no']]\n",
    "\n",
    "if yes_prob > no_prob:\n",
    "    k = 'yes'\n",
    "    prob = yes_prob\n",
    "else:\n",
    "    k = 'no'\n",
    "    prob = no_prob\n",
    "\n",
    "print(\"Predicted answer is: \", k)\n",
    "print(\"Probability of certainty was: \", round(prob*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('qa_bot')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b59e86b0cddfd01d82ea01636af62499c60e52215df1bfce637c1cca8b99496"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
